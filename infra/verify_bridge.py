#!/usr/bin/env python3
"""
The_Bridge Verification Script for Cursor
Runs smoke tests to verify RLS, event flow, and memory bank
"""

import os
import sys
import json
import time
import psycopg2
from psycopg2.extras import RealDictCursor
import requests
from datetime import datetime, timedelta
import numpy as np
import subprocess
import tempfile
from pathlib import Path

# Configuration
DB_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': os.getenv('POSTGRES_PORT', 5432),
    'database': os.getenv('POSTGRES_DB', 'continuum'),
    'user': os.getenv('POSTGRES_USER', 'bridge_admin'),
    'password': os.getenv('POSTGRES_PASSWORD', 'bridge_secure_2025')
}

KAFKA_CONFIG = {
    'bootstrap_servers': os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:19092'),
    'schema_registry': os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:18081')
}

MINIO_CONFIG = {
    'endpoint': os.getenv('MINIO_ENDPOINT', 'localhost:9000'),
    'access_key': os.getenv('MINIO_ACCESS_KEY', 'bridge_admin'),
    'secret_key': os.getenv('MINIO_SECRET_KEY', 'bridge_secure_2025')
}

API_CONFIG = {
    'memory_bank_url': os.getenv('MEMORY_BANK_URL', 'http://localhost:8088'),
    'dbt_project_dir': '/mnt/c/users/password/continuum_Overworld/Forge/DataPlatform--DBT__DEV@v0.1.0'
}

class BridgeVerifier:
    def __init__(self):
        self.results = []
        self.passed = 0
        self.failed = 0
        
    def connect_db(self, user='bridge_admin', password='bridge_secure_2025'):
        """Connect to database with specified user"""
        config = DB_CONFIG.copy()
        config['user'] = user
        config['password'] = password
        return psycopg2.connect(**config, cursor_factory=RealDictCursor)
    
    def test(self, name, func):
        """Run a test and track results"""
        print(f"\nüîç Testing: {name}")
        try:
            result = func()
            if result:
                print(f"  ‚úÖ PASSED")
                self.passed += 1
                self.results.append({'test': name, 'status': 'PASSED'})
            else:
                print(f"  ‚ùå FAILED")
                self.failed += 1
                self.results.append({'test': name, 'status': 'FAILED'})
            return result
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
            self.failed += 1
            self.results.append({'test': name, 'status': 'ERROR', 'error': str(e)})
            return False
    
    def verify_rls_isolation(self):
        """Test 1: RLS Tenant Isolation"""
        print("\n" + "="*60)
        print("TEST 1: RLS ISOLATION")
        print("="*60)
        
        # Setup test data as admin
        with self.connect_db() as conn:
            with conn.cursor() as cur:
                # Insert test data for multiple tenants
                cur.execute("""
                    INSERT INTO core.document (doc_id, tenant_id, doc_type, title)
                    VALUES 
                        ('doc_gsg_1', 'GSG', 'test', 'GSG Document 1'),
                        ('doc_gsg_2', 'GSG', 'test', 'GSG Document 2'),
                        ('doc_demo_1', 'DEMO', 'test', 'DEMO Document 1'),
                        ('doc_demo_2', 'DEMO', 'test', 'DEMO Document 2')
                    ON CONFLICT (doc_id) DO NOTHING
                """)
                conn.commit()
        
        # Test as GSG user
        def test_gsg_isolation():
            with self.connect_db('app_gsg', 'gsg_secure_2025') as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT set_config('app.tenant_id', 'GSG', true)")
                    cur.execute("SELECT COUNT(*) as count FROM core.document WHERE doc_type = 'test'")
                    gsg_count = cur.fetchone()['count']
                    
                    cur.execute("SELECT doc_id FROM core.document WHERE doc_type = 'test'")
                    gsg_docs = [row['doc_id'] for row in cur.fetchall()]
                    
                    # Should only see GSG documents
                    return gsg_count == 2 and all('gsg' in doc for doc in gsg_docs)
        
        # Test as DEMO user
        def test_demo_isolation():
            with self.connect_db('app_demo', 'demo_secure_2025') as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT set_config('app.tenant_id', 'DEMO', true)")
                    cur.execute("SELECT COUNT(*) as count FROM core.document WHERE doc_type = 'test'")
                    demo_count = cur.fetchone()['count']
                    
                    cur.execute("SELECT doc_id FROM core.document WHERE doc_type = 'test'")
                    demo_docs = [row['doc_id'] for row in cur.fetchall()]
                    
                    # Should only see DEMO documents
                    return demo_count == 2 and all('demo' in doc for doc in demo_docs)
        
        # Test cross-tenant insert (should fail)
        def test_cross_tenant_insert():
            with self.connect_db('app_gsg', 'gsg_secure_2025') as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT set_config('app.tenant_id', 'GSG', true)")
                    try:
                        cur.execute("""
                            INSERT INTO core.document (doc_id, tenant_id, doc_type, title)
                            VALUES ('doc_hack', 'DEMO', 'test', 'Hacked Document')
                        """)
                        conn.commit()
                        return False  # Should not succeed
                    except:
                        conn.rollback()
                        return True  # Expected to fail
        
        self.test("GSG tenant sees only GSG data", test_gsg_isolation)
        self.test("DEMO tenant sees only DEMO data", test_demo_isolation)
        self.test("Cross-tenant insert blocked", test_cross_tenant_insert)
    
    def verify_event_roundtrip(self):
        """Test 2: Event Round-trip"""
        print("\n" + "="*60)
        print("TEST 2: EVENT ROUND-TRIP")
        print("="*60)
        
        # Create test event
        event = {
            "headers": {
                "world": "Continuum_Overworld",
                "division": "Oracle",
                "capability": "Calculator",
                "version": "v1.0.0",
                "tenant_id": "GSG",
                "project_tag": "TEST-001",
                "occurred_at": datetime.utcnow().isoformat(),
                "payload_schema": "esg.metric.v1"
            },
            "payload": {
                "doc_id": "test_doc_001",
                "metrics": [{
                    "metric_type": "scope1",
                    "metric_name": "Direct Emissions",
                    "value": 1234.56,
                    "unit": "tCO2e",
                    "confidence": 0.95
                }]
            }
        }
        
        def test_event_storage():
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Insert contract
                    cur.execute("""
                        INSERT INTO bridge.contracts (contract_name, version, schema_json)
                        VALUES ('esg.metric.v1', 1, %s)
                        ON CONFLICT (contract_name, version) DO NOTHING
                    """, (json.dumps({}),))
                    
                    # Insert event
                    cur.execute("""
                        INSERT INTO bridge.event_registry 
                        (event_type, contract_name, version, tenant_id, project_tag, 
                         occurred_at, payload, headers)
                        VALUES ('ESG_METRIC_EXTRACTED', 'esg.metric.v1', 1, %s, %s, %s, %s, %s)
                        RETURNING event_id
                    """, (
                        event['headers']['tenant_id'],
                        event['headers']['project_tag'],
                        event['headers']['occurred_at'],
                        json.dumps(event['payload']),
                        json.dumps(event['headers'])
                    ))
                    
                    event_id = cur.fetchone()['event_id']
                    conn.commit()
                    
                    # Verify storage
                    cur.execute("SELECT * FROM bridge.event_registry WHERE event_id = %s", (event_id,))
                    stored = cur.fetchone()
                    
                    return stored is not None and stored['tenant_id'] == 'GSG'
        
        def test_event_to_core():
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Simulate event processing to core tables
                    metric = event['payload']['metrics'][0]
                    cur.execute("""
                        INSERT INTO core.esg_metric 
                        (tenant_id, doc_id, metric_type, metric_name, value, unit, confidence)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        RETURNING metric_id
                    """, (
                        event['headers']['tenant_id'],
                        event['payload']['doc_id'],
                        metric['metric_type'],
                        metric['metric_name'],
                        metric['value'],
                        metric['unit'],
                        metric['confidence']
                    ))
                    
                    metric_id = cur.fetchone()['metric_id']
                    conn.commit()
                    return metric_id is not None
        
        self.test("Event stored in registry", test_event_storage)
        self.test("Event processed to core tables", test_event_to_core)
    
    def verify_embeddings(self):
        """Test 3: Vector Embeddings"""
        print("\n" + "="*60)
        print("TEST 3: EMBEDDINGS & VECTOR SEARCH")
        print("="*60)
        
        def test_embedding_storage():
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Create sample embeddings (1536 dimensions like OpenAI)
                    docs = [
                        ("emb_1", "GSG", "Sustainability Report 2024", np.random.randn(1536)),
                        ("emb_2", "GSG", "Carbon Emissions Analysis", np.random.randn(1536)),
                        ("emb_3", "GSG", "Water Usage Report", np.random.randn(1536))
                    ]
                    
                    for doc_id, tenant_id, title, embedding in docs:
                        cur.execute("""
                            INSERT INTO core.memory_doc 
                            (doc_id, tenant_id, scope, title, content, embedding)
                            VALUES (%s, %s, 'global', %s, %s, %s)
                            ON CONFLICT (doc_id) DO NOTHING
                        """, (doc_id, tenant_id, title, f"Content for {title}", embedding.tolist()))
                    
                    conn.commit()
                    
                    # Verify storage
                    cur.execute("SELECT COUNT(*) as count FROM core.memory_doc WHERE doc_id LIKE 'emb_%'")
                    return cur.fetchone()['count'] >= 3
        
        def test_vector_search():
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Create query embedding
                    query_embedding = np.random.randn(1536)
                    
                    # Search similar documents
                    cur.execute("""
                        SELECT doc_id, title,
                               1 - (embedding <=> %s::vector) as similarity
                        FROM core.memory_doc
                        WHERE tenant_id = 'GSG'
                        AND embedding IS NOT NULL
                        ORDER BY embedding <=> %s::vector
                        LIMIT 3
                    """, (query_embedding.tolist(), query_embedding.tolist()))
                    
                    results = cur.fetchall()
                    
                    # Should return 3 results with similarity scores
                    return len(results) == 3 and all(0 <= r['similarity'] <= 1 for r in results)
        
        self.test("Embedding storage", test_embedding_storage)
        self.test("Vector similarity search", test_vector_search)
    
    def verify_agent_trace(self):
        """Test 4: Agent Run Trace"""
        print("\n" + "="*60)
        print("TEST 4: AGENT RUN TRACE")
        print("="*60)
        
        def test_agent_run_storage():
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Create agent run
                    run_id = f"run_{int(time.time())}"
                    started = datetime.utcnow()
                    ended = started + timedelta(seconds=5)
                    
                    cur.execute("""
                        INSERT INTO core.agent_run
                        (agent_run_id, tenant_id, agent_name, project_tag,
                         input, output, tools, status, started_at, ended_at, duration_ms)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING agent_run_id
                    """, (
                        run_id, 'GSG', 'Test_Agent', 'TEST-001',
                        json.dumps({"prompt": "Test prompt"}),
                        json.dumps({"response": "Test response"}),
                        json.dumps([{
                            "tool_name": "memory_search",
                            "duration_ms": 150,
                            "status": "success"
                        }]),
                        'success', started, ended, 5000
                    ))
                    
                    stored_id = cur.fetchone()['agent_run_id']
                    conn.commit()
                    
                    # Create memory trace event
                    cur.execute("""
                        INSERT INTO bridge.event_registry
                        (event_type, tenant_id, agent_run_id, occurred_at, payload)
                        VALUES ('AGENT_RUN_RECORDED', %s, %s, %s, %s)
                    """, ('GSG', run_id, datetime.utcnow(), json.dumps({"run_id": run_id})))
                    
                    conn.commit()
                    return stored_id == run_id
        
        def test_agent_context():
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Get agent context
                    cur.execute("SELECT core.get_agent_context('Test_Agent', 'GSG', 24) as context")
                    context = cur.fetchone()['context']
                    
                    # Should have recent runs
                    return context is not None and 'recent_runs' in context
        
        self.test("Agent run storage", test_agent_run_storage)
        self.test("Agent context retrieval", test_agent_context)
    
    def verify_reference_data(self):
        """Test 5: Reference Data Access"""
        print("\n" + "="*60)
        print("TEST 5: SHARED REFERENCE DATA")
        print("="*60)
        
        def test_defra_factors():
            with self.connect_db('app_gsg', 'gsg_secure_2025') as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT set_config('app.tenant_id', 'GSG', true)")
                    
                    # Should be able to read reference data
                    cur.execute("""
                        SELECT COUNT(*) as count 
                        FROM reference.defra_factors_v2025
                        WHERE mode = 'truck'
                    """)
                    truck_factors = cur.fetchone()['count']
                    
                    return truck_factors > 0
        
        def test_reference_readonly():
            with self.connect_db('app_gsg', 'gsg_secure_2025') as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT set_config('app.tenant_id', 'GSG', true)")
                    
                    # Should NOT be able to modify reference data
                    try:
                        cur.execute("""
                            INSERT INTO reference.defra_factors_v2025 
                            (mode, vehicle_class, unit, co2e_per_unit)
                            VALUES ('test', 'test', 'test', 0.1)
                        """)
                        conn.commit()
                        return False  # Should not succeed
                    except:
                        conn.rollback()
                        return True  # Expected to fail
        
        self.test("Read DEFRA factors", test_defra_factors)
        self.test("Reference data is read-only", test_reference_readonly)
    
    def verify_dbt_models(self):
        """Test 6: dbt Models Build"""
        print("\n" + "="*60)
        print("TEST 6: DBT MODELS & TRANSFORMATIONS")
        print("="*60)
        
        def test_dbt_deps():
            dbt_dir = Path(API_CONFIG['dbt_project_dir'])
            if not dbt_dir.exists():
                return False
                
            try:
                result = subprocess.run(
                    ['dbt', 'deps', '--profiles-dir', str(dbt_dir)],
                    cwd=dbt_dir,
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                return result.returncode == 0
            except (subprocess.TimeoutExpired, FileNotFoundError):
                return False
        
        def test_dbt_compile():
            dbt_dir = Path(API_CONFIG['dbt_project_dir'])
            if not dbt_dir.exists():
                return False
                
            try:
                result = subprocess.run(
                    ['dbt', 'compile', '--profiles-dir', str(dbt_dir)],
                    cwd=dbt_dir,
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                return result.returncode == 0
            except (subprocess.TimeoutExpired, FileNotFoundError):
                return False
        
        def test_bronze_model_exists():
            dbt_dir = Path(API_CONFIG['dbt_project_dir'])
            bronze_model = dbt_dir / 'models' / 'bronze' / 'bronze_esg_metric_events.sql'
            return bronze_model.exists()
        
        def test_silver_model_exists():
            dbt_dir = Path(API_CONFIG['dbt_project_dir'])
            silver_model = dbt_dir / 'models' / 'silver' / 'silver_fct_esg_metric.sql'
            return silver_model.exists()
        
        self.test("dbt dependencies install", test_dbt_deps)
        self.test("dbt models compile", test_dbt_compile)
        self.test("Bronze models exist", test_bronze_model_exists)
        self.test("Silver models exist", test_silver_model_exists)
    
    def verify_memory_bank_api(self):
        """Test 7: Memory Bank REST API"""
        print("\n" + "="*60)
        print("TEST 7: MEMORY BANK REST API")
        print("="*60)
        
        base_url = API_CONFIG['memory_bank_url']
        
        def test_api_health():
            try:
                response = requests.get(f"{base_url}/health", timeout=5)
                return response.status_code == 200
            except requests.RequestException:
                return False
        
        def test_api_root():
            try:
                response = requests.get(f"{base_url}/", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    return data.get('service') == 'The_Bridge Memory Bank API'
                return False
            except requests.RequestException:
                return False
        
        def test_memory_doc_upsert():
            try:
                headers = {'X-Tenant-ID': 'GSG', 'Content-Type': 'application/json'}
                doc_data = {
                    'doc_id': 'test_api_doc_1',
                    'scope': 'global',
                    'title': 'Test API Document',
                    'content': 'This is a test document for API verification.',
                    'metadata': {'test': True}
                }
                
                response = requests.post(
                    f"{base_url}/v1/memory/doc",
                    json=doc_data,
                    headers=headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get('doc_id') == 'test_api_doc_1'
                return False
            except requests.RequestException:
                return False
        
        def test_memory_search():
            try:
                headers = {'X-Tenant-ID': 'GSG'}
                response = requests.get(
                    f"{base_url}/v1/memory/search",
                    params={'q': 'test document', 'k': 3},
                    headers=headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return 'results' in data and 'query' in data
                return False
            except requests.RequestException:
                return False
        
        def test_kv_store():
            try:
                headers = {'X-Tenant-ID': 'GSG', 'Content-Type': 'application/json'}
                kv_data = {
                    'key': 'test_api_key',
                    'value': {'message': 'Hello from API test'},
                    'scope': 'global',
                    'ttl_seconds': 300
                }
                
                # Set value
                response = requests.post(
                    f"{base_url}/v1/kv",
                    json=kv_data,
                    headers=headers,
                    timeout=5
                )
                
                if response.status_code != 200:
                    return False
                
                # Get value
                response = requests.get(
                    f"{base_url}/v1/kv/test_api_key",
                    headers=headers,
                    timeout=5
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get('key') == 'test_api_key'
                return False
            except requests.RequestException:
                return False
        
        def test_tenant_isolation():
            try:
                # Try to access GSG data with DEMO tenant header
                headers = {'X-Tenant-ID': 'DEMO'}
                response = requests.get(
                    f"{base_url}/v1/memory/search",
                    params={'q': 'test document', 'k': 3},
                    headers=headers,
                    timeout=5
                )
                
                if response.status_code == 200:
                    data = response.json()
                    # Should not find GSG documents
                    return len(data.get('results', [])) == 0 or \
                           all('test_api_doc_1' not in r.get('doc_id', '') for r in data['results'])
                return False
            except requests.RequestException:
                return False
        
        self.test("API health endpoint", test_api_health)
        self.test("API root endpoint", test_api_root)
        self.test("Memory document upsert", test_memory_doc_upsert)
        self.test("Memory vector search", test_memory_search)
        self.test("KV store operations", test_kv_store)
        self.test("API tenant isolation", test_tenant_isolation)
    
    def verify_producer_consumer_flow(self):
        """Test 8: Event Producer/Consumer Flow"""
        print("\n" + "="*60)
        print("TEST 8: PRODUCER/CONSUMER PIPELINE")
        print("="*60)
        
        def test_csr_producer_module():
            producer_path = Path('/mnt/c/users/password/continuum_Overworld/Forge/Ingestor--CSR__EU-DE@v1/producer.py')
            return producer_path.exists()
        
        def test_esg_consumer_module():
            consumer_path = Path('/mnt/c/users/password/continuum_Overworld/Oracle/Calculator--ESG__PROD@v1/consumer.py')
            return consumer_path.exists()
        
        def test_lake_ingestor_module():
            ingestor_path = Path('/mnt/c/users/password/continuum_Overworld/Forge/LakeIngestor--Events__DEV@v0.1.0/lake_ingestor.py')
            return ingestor_path.exists()
        
        def test_csr_to_esg_pipeline():
            """Test that CSR ingestion creates documents that ESG consumer can process"""
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Insert a test CSR document
                    test_content = """
                    GreenStem Sustainability Report 2024
                    Scope 1 emissions: 5,200 tCO2e
                    Water usage: 125,000 m¬≥
                    Renewable energy: 67%
                    """
                    
                    doc_id = f"test_csr_{int(time.time())}"
                    cur.execute("""
                        INSERT INTO core.document 
                        (doc_id, tenant_id, doc_type, title, extracted_text, processing_status)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (doc_id, 'GSG', 'csr_report', 'Test CSR Report', test_content, 'ingested'))
                    
                    conn.commit()
                    
                    # Verify document was created
                    cur.execute("SELECT doc_id FROM core.document WHERE doc_id = %s", (doc_id,))
                    result = cur.fetchone()
                    
                    return result is not None
        
        def test_esg_metric_extraction_simulation():
            """Test ESG metric extraction logic"""
            test_content = """
            Environmental Performance Report
            Direct emissions (Scope 1): 15,678 tCO2e
            Purchased electricity (Scope 2): 8,432 tCO2e  
            Value chain emissions (Scope 3): 245,678 tCO2e
            Water consumption: 89,000 m¬≥
            Renewable energy: 78%
            """
            
            # Import extraction function (simulate the consumer logic)
            import re
            
            scope_patterns = {
                'scope1': [r'scope\s*1[:\s]+([0-9,]+(?:\.[0-9]+)?)\s*tco2?e?'],
                'scope2': [r'scope\s*2[:\s]+([0-9,]+(?:\.[0-9]+)?)\s*tco2?e?'],
                'scope3': [r'scope\s*3[:\s]+([0-9,]+(?:\.[0-9]+)?)\s*tco2?e?']
            }
            
            extracted_metrics = []
            for scope, patterns in scope_patterns.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, test_content, re.IGNORECASE)
                    for match in matches:
                        value_str = match.group(1).replace(',', '')
                        try:
                            value = float(value_str)
                            extracted_metrics.append({
                                'metric_type': scope,
                                'value': value,
                                'unit': 'tCO2e'
                            })
                        except ValueError:
                            continue
            
            # Should extract 3 scope metrics
            return len(extracted_metrics) == 3
        
        self.test("CSR Producer module exists", test_csr_producer_module)
        self.test("ESG Consumer module exists", test_esg_consumer_module)
        self.test("Lake Ingestor module exists", test_lake_ingestor_module)
        self.test("CSR document pipeline", test_csr_to_esg_pipeline)
        self.test("ESG metric extraction logic", test_esg_metric_extraction_simulation)
    
    def verify_lake_storage(self):
        """Test 9: Lake Storage Structure"""
        print("\n" + "="*60)
        print("TEST 9: LAKE STORAGE & PARTITIONING")
        print("="*60)
        
        def test_parquet_structure():
            """Test that the partitioning structure is correct"""
            try:
                import s3fs
                s3 = s3fs.S3FileSystem(
                    endpoint_url=f"http://{MINIO_CONFIG['endpoint']}",
                    key=MINIO_CONFIG['access_key'],
                    secret=MINIO_CONFIG['secret_key'],
                    use_ssl=False
                )
                
                # Check if lake bucket exists
                try:
                    s3.ls('lake/')
                    return True
                except FileNotFoundError:
                    return False
                    
            except ImportError:
                # s3fs not available, skip test
                return True
        
        def test_event_partitioning_logic():
            """Test the partitioning logic without actual storage"""
            from datetime import datetime
            
            def get_partition_path(event, topic):
                headers = event.get('headers', {})
                tenant_id = headers.get('tenant_id', 'UNKNOWN')
                project_tag = headers.get('project_tag', 'GLOBAL')
                
                occurred_at = headers.get('occurred_at', datetime.utcnow().isoformat())
                try:
                    dt = datetime.fromisoformat(occurred_at.replace('Z', '+00:00'))
                    ds = dt.date().isoformat()
                except (ValueError, AttributeError):
                    ds = datetime.utcnow().date().isoformat()
                
                topic_clean = topic.replace('/', '_').replace(' ', '_')
                return (topic_clean, tenant_id, project_tag, ds)
            
            # Test event
            event = {
                'headers': {
                    'tenant_id': 'GSG',
                    'project_tag': 'TEST-001',
                    'occurred_at': '2024-01-15T10:30:00Z'
                }
            }
            
            partition = get_partition_path(event, 'continuum.events')
            expected = ('continuum.events', 'GSG', 'TEST-001', '2024-01-15')
            
            return partition == expected
        
        self.test("MinIO/S3 connectivity", test_parquet_structure)
        self.test("Event partitioning logic", test_event_partitioning_logic)
    
    def print_summary(self):
        """Print test summary"""
        print("\n" + "="*60)
        print("VERIFICATION SUMMARY")
        print("="*60)
        
        total = self.passed + self.failed
        print(f"\nüìä Results: {self.passed}/{total} tests passed")
        
        if self.failed == 0:
            print("‚úÖ ALL TESTS PASSED - The_Bridge is operational!")
        else:
            print(f"‚ùå {self.failed} tests failed - review logs above")
        
        print("\nüìã Test Details:")
        for result in self.results:
            status_icon = "‚úÖ" if result['status'] == 'PASSED' else "‚ùå"
            print(f"  {status_icon} {result['test']}: {result['status']}")
            if 'error' in result:
                print(f"     Error: {result['error']}")
        
        return self.failed == 0

def main():
    """Main verification routine"""
    print("üåâ THE_BRIDGE VERIFICATION SUITE")
    print("="*60)
    print("Verifying multi-tenant data foundation...")
    
    verifier = BridgeVerifier()
    
    # Run all verification tests
    verifier.verify_rls_isolation()
    verifier.verify_event_roundtrip()
    verifier.verify_embeddings()
    verifier.verify_agent_trace()
    verifier.verify_reference_data()
    verifier.verify_dbt_models()
    verifier.verify_memory_bank_api()
    verifier.verify_producer_consumer_flow()
    verifier.verify_lake_storage()
    
    # Print summary
    success = verifier.print_summary()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()